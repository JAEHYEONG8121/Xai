# LIME (Local Interpretable Model-Agnostic Explanations)

ğŸ“¢ **PyTorch Implementation Based on the Original Paper**  
This project is based on the paper ["Why Should I Trust You?" (Ribeiro et al., 2016)](https://arxiv.org/abs/1602.04938)  
and implements LIME using **PyTorch** from scratch.

## 1. What is LIME?
LIME (Local Interpretable Model-Agnostic Explanations) is a method for **explaining the predictions of machine learning models**.  
It can be applied to any model and helps interpret its predictions.

**ğŸŒŸ Key Principles**  
1ï¸âƒ£ **Perturbation** â†’ Modify the original data to generate new samples.  
2ï¸âƒ£ **Weighting** â†’ Assign higher weights to samples closer to the original data.  
3ï¸âƒ£ **Local Model Training** â†’ Train a linear regression model to improve interpretability.  

## 2. Quick Start
### 1ï¸âƒ£ **Set Up Virtual Environment & Install Dependencies (Anaconda)**
```bash
# Create a new virtual environment
conda create --name lime_env python=3.8

# Activate the environment
conda activate lime_env

# Install required packages
pip install -r requirements.txt
```

## 3. Project Directory Path

LIME-PyTorch/
â”‚â”€â”€ README.md                 
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ main.py
â”‚â”€â”€ lime_experiment.ipynb     
â”‚â”€â”€ src/                      
â”‚   â”‚â”€â”€ model.py              
â”‚   â”‚â”€â”€ lime.py               
â”‚   â”‚â”€â”€ utils.py              
â”‚â”€â”€ results/                  
â”‚   â””â”€â”€ feature_importance.png
