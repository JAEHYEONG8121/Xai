# LIME (Local Interpretable Model-Agnostic Explanations)

üì¢ **PyTorch Implementation Based on the Original Paper**  
This project is based on the paper ["Why Should I Trust You?" (Ribeiro et al., 2016)](https://arxiv.org/abs/1602.04938)  
and implements LIME using **PyTorch** from scratch.

## 1. What is LIME?
LIME (Local Interpretable Model-Agnostic Explanations) is a method for **explaining the predictions of machine learning models**.  
It can be applied to any model and helps interpret its predictions.

**üåü Key Principles**  
1Ô∏è‚É£ **Perturbation** ‚Üí Modify the original data to generate new samples.  
2Ô∏è‚É£ **Weighting** ‚Üí Assign higher weights to samples closer to the original data.  
3Ô∏è‚É£ **Local Model Training** ‚Üí Train a linear regression model to improve interpretability.  

## 2. Quick Start
### 1Ô∏è‚É£ **Set Up Virtual Environment & Install Dependencies (Anaconda)**
```bash
# Create a new virtual environment
conda create --name lime_env python=3.8

# Activate the environment
conda activate lime_env

# Install required packages
pip install -r requirements.txt
